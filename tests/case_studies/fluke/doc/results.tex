%
%
%

\section{Results}
\label{results}

In addition to the descriptive models, and a greater understanding of
the Fluke IPC code, this project generated quite a bit of
documentation of the kernel internals. We have static call traces of
the IPC system from the user-level entry point down into the core
kernel routines. We used fully cross-referenced html versions of the
kernel code~\cite{godmars-html}.

The most important results are those from the SPIN generated
verifications of various scenarios.  Because we modeled the kernel
entry layer accurately we are able to
generate simple test cases that are easy to code yet quite flexible.

A standard test case will enumerate the list of kernel entry-points it
plans to use. The preprocessor will ensure that only the need parts
of the Promela IPC implementation are included, dramatically reducing
compile time by omitting all unused code. One proctype is created for 
each user-level thread. These proctypes can then ``call'' user-level 
fluke functions by macro expansion. Consider the basic send-and-receive
test as an example: 

\begin{verbatim}
  do
  :: TRUE ->
       sendData = 42;
       flukeClientConnectSend(fluke, server, sendData);
       assert(rc == 0);

       flukeClientDisconnect(fluke);
       assert(rc == 0);
  od;
\end{verbatim}

The {\tt fluke} parameter is a handle to a proctype implementing the 
kernel.  All of our functions implicitly set a global variable 
{\tt rc}, Promela's {\tt assert()} construct is then used to verify
successful completion.

Certain simulation parameters are configurable through the use
of preprocessor constants before including the Promela files that
contain the definitions of the IPC model. At this point, there are three
parameters which can be set. First, one can select whether or not 
a simulated transfer can generate a page fault. Every transfer can 
cause a page fault in either the sender, or the receive, or it cannot
cause a page fault at all. This is controlled with 
{\tt INCLUDE_IPC_TRANSFER_FAULTS}.

Secondly, {\tt INCLUDE_IPC_PAYLOAD} determines whether IPC data payload
should be included in the simulation. With this option, a single byte
of data is transferred from the sender to the receiver. This lets us
assert that data hasn't been altered during the IPC transport.

Lastly, it is possible to choose between two distinct models of 
mutexes in Promela (see section~\ref{mutex-model}.) 
They differ in the number of checks and the amount of state 
associated with each.  The ``simple'' mutexes are a minimal 
implementation without any consistency checks. 
The ``safe'' implementation includes safety checks (via {\tt assert()})
and requires some extra state to be stored with each mutex. 
It is used if {\tt INCLUDE_MUTEX_SAFE} is defined.

%%
%% For each test:
%%   Basic desc.
%%   What it touches
%%   SPIN results
%%   Limitations.

%%%%%%%% Basic-send
\subsection{The Basic Send Test}

This test, {\tt src/test/Basic-send.pr}, is the first real test we
consider here.  It is is a very basic send-and-receive operation
involving two threads, a client and a server.  The client repeatedly
connects, sends a byte of data, and disconnects.  The server waits,
receives the byte of data, disconnects and then starts over.

%
% please allude earlier to the roll-back aspect
%
This tests the kernel entry layer, mutexes, the IPC connection
logic; it also tests a bit of the rollback, as threads effectively
rollback when moving from, for example, the ``wait'' to the
``receive'' in a ``wait_receive'' operation.  If IPC page fault
modeling is activated then arbitrary transfers will cause a page
fault in the client or the receiver. Our verification runs show
that the IPC system fully recovers from such faults.

A full verification run of this test takes about 41 MB of memory.
Interestingly, it takes longer to compile the test than it does to run
it.  It generates 94,000 states, and only executes to a maximum depth of 1650.
These are especially interesting results considering that over 60,000
lines of C code are generated by SPIN.
Because the send-receive is a fairly lockstep
operation--the server block until the client sends, the client blocks
until the server acks, etc--it is easy to see why there is little 
interleaving among the states.

This test does not cover other ``corner cases'' than the interleaving 
of page faults in the code and the recovery steps involved, in
particular it does not consider the case where either thread 
involved is canceled.

%%%%%%%% Ack
\subsection{The Ack Test}

This test, {\tt src/test/Ack-test.pr}, exercises the acknowledgment 
portion of the protocol.  Sending an ack tells the other half of your
connection that you have finished receiving and are ready to send data
back across the link.  

This tests much of the code exercised by the basic send test, in
addition the client send and ack-specific codes.

The scope of this test is similar to that of the basic send
test, it requires approximately 20 MB of memory to run. Again, the
compilation of the verifier takes longer than the verification itself.

This test has the same limitations as basic send, i.e., error 
conditions outside of a page fault are not considered.  Still, all
of the waiting, blocking and capturing occurs correctly in all
instances of page-faults and all possible arrival ordering. 

%%%%% Interposer
\subsection{The Interposer Test}

This is a more complicated test.  It involves three threads.  One is a
client whose server thread is a client to a third thread.   The
thread in the middle is acting as an interposer--it receives data from the
client and forwards it to the server.

This tests the middle thread's code path quite rigorously.  It is
acting as both a client and a server and can experience page faults on
either side of its connection.

As a more complicated test, this would require about 400 MB of memory
to hold all of its states.  (Its has about 10 million states.)  
Supertrace lets us run it in 130 MB of memory.

% Cancel-test.pr
\subsection{The Cancel Test}

This test, {\tt src/test/Cancel-test.pr}, was our ultimate goal for the
project.  We hoped to show that the exportable kernel state (which is
implemented via the thread cancel facility which allows a thread's
state to be extracted at any time) was robust.  It involves three
threads.  Two threads operate as client and server as in the basic
send test, endlessly connecting and sending data.  The third thread
sits in an infinite loop and cancels either the client or the server.  

While we haven't been able to run a complete successful verification
of this test, we still believe our model to be sound.  Currently
the only known deadlock is encountered 900 steps into a
verification. It is due entirely to a poor abstraction of ports and
port sets.  Specifically, we are not modeling the wait state, wait
queue and cancelability of threads that are waiting to connect to
their client or their server.  The initial abstraction we implemented
(a single bit for rendezvous) was too naieve.  The IPC code expects
the threads returned from a rendezvous to be in certain states.  It
also expects that the cancel pending flag will have been checked.  
This should be fixed as we continue to
maintain and expand the system for the Flux project.

This test is quite large. A complete state space exploration would
require on the order of 2 GB of memory.  The supertrace option in SPIN
lets it run in about 200 MB. The estimated coverage of the state
space is 99.9\%.  

\subsection{Other Tests}

We developed our Promela in a bottom-up fashion, starting with the
primitives used to implement the more complicated IPC operations.
We have a full suite of tests that test our specific abstractions--wait 
queues, condition variables, mutexes, the kentry layer, etc. 

\subsection{Test Analysis}

While debugging these tests, we have so far exclusively encountered 
bugs in our model of the system.  It is interesting to note that none 
of these bugs have been in the parts of the model that were derived
from the C code.  They have all been in those areas of the 
model where the C code could not be literally translated into Promela,
but where abstractions with equivalent semantics had to be reimplemented 
for efficiency and other reasons.  For example, a bug in
our model did not include checking for a pending cancel before blocking.  
This quickly broke the cancel test (i.e., within 100 steps.) 

